{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Torch's tensor libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating tensors \n",
    "torch.Tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n"
     ]
    }
   ],
   "source": [
    "# 1D vector\n",
    "vec_data = [1., 2., 3.]\n",
    "vec = torch.Tensor(vec_data)\n",
    "print(vec)\n",
    "\n",
    "#create matrix\n",
    "mat_data = [[1.,2.,3.], [4.,5.,6.]]\n",
    "M = torch.Tensor(mat_data)\n",
    "print(M)\n",
    "\n",
    "#creates a 3D tensor of size 2x2\n",
    "T_data = [[[1.,2.],[3.,4.]],[[5.,6.],[7.,8.]]]\n",
    "T = torch.Tensor(T_data)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor([1., 2., 3.])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "#Index into V get a scalar\n",
    "print(vec[0])\n",
    "\n",
    "#Index in M get a vector\n",
    "print(M[0])\n",
    "\n",
    "#Index in T and get matirx\n",
    "print(T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0986,  0.1418,  1.8484,  0.7456, -0.4697],\n",
       "         [ 0.7196,  0.6807,  0.2613, -0.0581, -1.9292],\n",
       "         [ 1.0769,  0.3044, -0.9771,  0.6452, -0.8265],\n",
       "         [ 0.5110,  0.6283, -0.3448, -1.5382,  1.3169]],\n",
       "\n",
       "        [[-0.3230,  1.1768,  0.4494, -0.4867, -0.6272],\n",
       "         [ 1.3033, -0.3181, -1.0643,  0.4012,  0.6666],\n",
       "         [-0.6953, -0.6589, -0.2782,  0.3641, -1.0407],\n",
       "         [ 0.0546,  0.8572, -1.9430,  1.1409,  0.1295]],\n",
       "\n",
       "        [[ 0.3445,  0.2413,  0.1938,  0.2094,  1.5771],\n",
       "         [ 0.1517, -0.5234,  1.4689, -0.2187, -0.1143],\n",
       "         [ 0.0629,  0.4502,  1.3818, -0.0603, -0.7738],\n",
       "         [ 2.5325, -0.7391,  0.5957, -0.7689, -0.0164]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((3,4,5))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations with Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([1.,2.,3])\n",
    "y = torch.Tensor([4.,5.,6.])\n",
    "z = x+y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2666,  0.2738,  0.9295, -0.5410, -0.4918],\n",
      "        [ 0.0789,  0.6938,  0.6598, -0.0810,  0.1165],\n",
      "        [ 1.7411, -0.5096,  0.5782, -0.1834, -0.6779],\n",
      "        [ 1.0669,  0.7617,  0.5461,  0.0355,  0.3889],\n",
      "        [ 1.5023,  1.1451,  0.6678,  0.3963, -0.6543]])\n",
      "tensor([[ 0.5460, -1.0964,  1.0026, -0.1827,  1.0717,  0.6771,  0.9234,  0.3206],\n",
      "        [-0.7633,  0.1964,  1.3181, -0.8131, -0.7522,  0.8126, -1.4560,  1.1771]])\n"
     ]
    }
   ],
   "source": [
    "# Default concatenation is along rows[Same num col]\n",
    "x_1 = torch.randn(2,5)\n",
    "y_1 = torch.randn(3,5)\n",
    "z_1 = torch.cat([x_1,y_1])\n",
    "print(z_1)\n",
    "\n",
    "#Concate columns[same number of rows]\n",
    "x_2 = torch.randn(2,3)\n",
    "y_2 = torch.randn(2,5)\n",
    "z_2 = torch.cat([x_2,y_2],1)\n",
    "print(z_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0923,  0.7348, -1.3668, -0.7047],\n",
      "         [ 0.3518, -1.1965,  1.2728, -0.6265],\n",
      "         [-0.2134, -0.0736, -2.1381, -0.4731]],\n",
      "\n",
      "        [[-0.1775, -1.2510,  0.1474, -0.5376],\n",
      "         [-0.6031,  0.2670, -1.8217, -1.4385],\n",
      "         [ 0.0368, -0.0427, -0.8904, -1.9731]]])\n",
      "tensor([[-0.0923,  0.7348, -1.3668, -0.7047,  0.3518, -1.1965,  1.2728, -0.6265,\n",
      "         -0.2134, -0.0736, -2.1381, -0.4731],\n",
      "        [-0.1775, -1.2510,  0.1474, -0.5376, -0.6031,  0.2670, -1.8217, -1.4385,\n",
      "          0.0368, -0.0427, -0.8904, -1.9731]])\n",
      "tensor([[-0.0923,  0.7348, -1.3668, -0.7047,  0.3518, -1.1965,  1.2728, -0.6265,\n",
      "         -0.2134, -0.0736, -2.1381, -0.4731],\n",
      "        [-0.1775, -1.2510,  0.1474, -0.5376, -0.6031,  0.2670, -1.8217, -1.4385,\n",
      "          0.0368, -0.0427, -0.8904, -1.9731]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,3,4)\n",
    "print(x)\n",
    "\n",
    "print(x.view(2,12))# Reshape to 2x12\n",
    "print(x.view(2,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Computation Graphs and Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([5., 7., 9.])\n",
      "<AddBackward0 object at 0x7f49d7623668>\n"
     ]
    }
   ],
   "source": [
    "# Variables wrap tensor objects\n",
    "x = autograd.Variable( torch.Tensor([1.,2.,3.]), requires_grad=True)\n",
    "# You can access the data with .data attribute\n",
    "print(x.data)\n",
    "\n",
    "y = autograd.Variable( torch.Tensor([4.,5.,6.]), requires_grad=True)\n",
    "z = x + y\n",
    "print(z.data)\n",
    "\n",
    "# Z knows some extra data \n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21., grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x7f48e2026d30>\n"
     ]
    }
   ],
   "source": [
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7f48d6408128>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((2,2))\n",
    "y = torch.rand((2,2))\n",
    "z = x + y # Normal tensor types and backprop would not be possible\n",
    "\n",
    "var_x = autograd.Variable( x,requires_grad=True )\n",
    "var_y = autograd.Variable( y,requires_grad=True )\n",
    "var_z = var_x + var_y\n",
    "print(var_z.grad_fn) # Has enough information to compute the gradients\n",
    "\n",
    "var_z_data = var_z.data\n",
    "new_var_z = autograd.Variable(var_z_data)\n",
    "print(new_var_z.grad_fn) # Does not ahve information to compute gradients because it only store the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above method breaks the Variable chain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) DL building blocks: Affine maps, non-linearities and objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine maps\n",
    "f(x) = Ax + b ; A --> matrix , b and x --> vectors\n",
    "The parameters to be learnt are A and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5716,  0.2606, -0.1326,  0.3243,  1.1428],\n",
      "        [ 0.7893,  1.4740, -0.1124,  0.2300,  1.4949]])\n",
      "tensor([[ 0.1204, -0.3842, -0.2769],\n",
      "        [ 0.3618, -0.2795, -0.6070]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "lin = nn.Linear(5, 3) # Maps from R^5 to R^3, parameters A and b\n",
    "data = autograd.Variable( torch.randn(2,5) )\n",
    "print(data)\n",
    "print(lin(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linearities\n",
    "\n",
    "Consider we have two affine maps: f(x)=Ax+b and g(x)=Cx+d. Then f(g(x)) = f(Cx+d) = A(Cx+d)+b = ACx + (Ad+b) which is another affine map. This does not add anything new while computing the model hence we need non-linearities. That most common non-linearitites are tanh(x), ReLU(x), and σ(x) this is because their gradients are easy ot compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4266, -1.1660],\n",
      "        [-1.4285,  1.7592]])\n",
      "tensor([[0.0000, 0.0000],\n",
      "        [0.0000, 1.7592]])\n"
     ]
    }
   ],
   "source": [
    "data = autograd.Variable(torch.randn(2,2))\n",
    "print(data)\n",
    "print(F.relu(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax and Probabilities\n",
    "\n",
    "This is a special non-linearity that is used at the end of the network because it returns a probability distribution. The ith component of the Softmax(x) is   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5175, -0.6878, -0.3383,  1.1900,  0.5298])\n",
      "tensor([0.0341, 0.0783, 0.1110, 0.5120, 0.2646])\n",
      "tensor(1.)\n",
      "tensor([-3.3771, -2.5473, -2.1979, -0.6695, -1.3297])\n"
     ]
    }
   ],
   "source": [
    "data = autograd.Variable(torch.randn(5))\n",
    "print(data)\n",
    "print(F.softmax(data))\n",
    "print(F.softmax(data).sum())\n",
    "print(F.log_softmax(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "The Objective Function is the function being trained to minimize. It chooses an instance to run through the network and then updates parameters with the derivative of the loss function. Negative-log probabity is very common for multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Optimization and Training\n",
    "Since the loss is an autograd.Variable it has enough information to compute the gradient with respect to all parameters used to compute it. If L(θ) is the loss function and n is a positive learning rate then : <br>\n",
    "$$ \\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta) $$ <br>\n",
    "[torch.optim has different optimizer packages]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Creating Network Components in Pytorch\n",
    "Now we will use affine maps and non-linearities to build a network. We will compute the loss fucntion using the built in negative log likelihood and update using backpropagation.<br>\n",
    "\n",
    "All network components should inheret from nn.Module and override the forward() method. This makes it keep track of its trainable parameters and you can swap between GPU(.cuda()) and CPU(.cpu()) <br>\n",
    "\n",
    "Now we will write a logistic regression model that takes a sparse bag-of-words and outputs a probaility distribution over two labels \"English\" and \"Spanish\"   \n",
    "\n",
    "## Example : Logistic Regression BOW classifier\n",
    "\n",
    "The modle maps sparse BOW to log probabilities over lables and assign each word in the vocab to an index. Example \"hello\" and \"world\" are indices 0 and 1. The sentence \"hello hello world\" is <br>\n",
    "                                       $$ [2,1] ==> [count(hello),count(world)]$$\n",
    "If the BOW is a vector 'x' then the ouput of the network is: <br>\n",
    "                                        $$ logSoftmax(Ax+b) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 8, 'get': 20, 'not': 17, 'buena': 14, 'gusta': 1, 'en': 3, 'is': 16, 'si': 24, 'me': 0, 'No': 9, 'sea': 12, 'comer': 2, 'at': 22, 'good': 19, 'una': 13, 'it': 7, 'cafeteria': 5, 'creo': 10, 'on': 25, 'idea': 15, 'Give': 6, 'la': 4, 'a': 18, 'Yo': 23, 'que': 11, 'lost': 21}\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"me gusta comer en la cafeteria\".split(),\"SPANISH\"),\n",
    "         (\"Give it to me\".split(),\"ENGLISH\"),\n",
    "         (\"No creo que sea una buena idea\".split(),\"SPANISH\"),\n",
    "         (\"No it is not a good idea to get lost at sea\".split(),\"ENGLISH\") ]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(),\"SPANISH\"),\n",
    "             (\"it is lost on me\".split(),\"ENGLISH\")]\n",
    "\n",
    "#word_to_ix maps the words in vocab to unique integers which will be its index in the BOW vectors\n",
    "\n",
    "word_to_ix={}\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word]=len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module): #inheriting from nn.Module\n",
    "    \n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls init func of nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        \n",
    "        # Define parameters required A,b and Torch provides nn.Linear() for affine map\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "        \n",
    "    def forward(self, bow_vec):\n",
    "        # Pass the input through linear layer then softmax\n",
    "        # Many other non-linearitities are present in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence : \n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1,-1)\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1485, -0.0696,  0.0650, -0.0275,  0.0792,  0.0345,  0.0937,  0.0263,\n",
      "         -0.1935, -0.0933,  0.1928, -0.1467,  0.1259, -0.1769, -0.1576,  0.1131,\n",
      "         -0.1096, -0.1941, -0.1163, -0.0220,  0.1786,  0.0360, -0.0929,  0.0506,\n",
      "          0.0272,  0.1691],\n",
      "        [ 0.1182, -0.1236,  0.0987, -0.0111,  0.0800, -0.1806,  0.0618, -0.0493,\n",
      "          0.1459, -0.0584, -0.0824, -0.1255,  0.1581, -0.0390,  0.1376,  0.1786,\n",
      "         -0.1501, -0.1632, -0.0833, -0.0681, -0.0364, -0.0883,  0.1163, -0.0930,\n",
      "          0.1896, -0.0311]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0307, 0.0311], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "\n",
    "# The model knows its parameters the first ouput is A and then b\n",
    "# BoWClassifier will store the nn.Linear's parameters\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7180, -0.6689]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "sample = data[0]\n",
    "bow_vectors = make_bow_vector(sample[0],word_to_ix)\n",
    "log_probs = model(autograd.Variable(bow_vectors))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ix = {\"SPANISH\":0, \"ENGLISH\":1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass instances through the log probabilities compute loss function then gradient of loss function and update the parameters with gradient step. The nn.NLLLoss is the negative log likelihood loss. The input for it is vector of log probabilities and and the target labels. This does not compute the log probabilities but nn.CrossEntropyLoss() is the same as NLLLoss except it includes the log softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5827, -0.8174]], grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[-0.6103, -0.7835]], grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([ 0.1928, -0.0824], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "for instance, labels in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)\n",
    "print( next(model.parameters())[:,word_to_ix['creo']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        #Step1: Pytorch accumulates gradients, clear them before each instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        #Step2: Make our BOW vector and also we must wrap the target variable as an integer. 0->SPANISH\n",
    "        bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "        target = autograd.Variable(make_target(label,label_to_ix))\n",
    "        \n",
    "        #Step3: Run the forward pass\n",
    "        log_probs = model(bow_vec)\n",
    "        \n",
    "        #Step4: Compute the loss,gradients and update parameters\n",
    "   \n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0944, -2.4070]], grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[-2.2920, -0.1065]], grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([ 0.6421, -0.5317], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "for instance, label in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance,word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)\n",
    "print(next(model.parameters())[:,word_to_ix['creo']]) # Index corresponding to spanish goes up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Word Embeddings : Encoding Lexical Semantics\n",
    "Word embeddings are dense vectors of real numbers, one word per vocabulary. If we use one-hot encoding the vector might be large and sparse and thus would not provid erelevant information about the words. Thus word embeddings represent the semantics of the word, efficiently ecoding semantic information for the task. Word embedings are stored as $|V| \\times D$ in pytorch, where $D$ is the dimensionlity of the embeddings such that the word with index $i$ has it's information in the $i$th row matrix . torch.nn.Embedding uses embeddings which takes two arguments vocabulary size and dimensionality of embeddings. To index the table use torch.LongTensor since the indexes are integers not floats.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], size=(0, 5), grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {'hello':0,'world':1}\n",
    "embeds = nn.Embedding(2,5) # 2 words in a 5D embedding \n",
    "lookup_tensor = torch.LongTensor(word_to_ix[\"hello\"])\n",
    "hello_embed = embeds(autograd.Variable(lookup_tensor))\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
